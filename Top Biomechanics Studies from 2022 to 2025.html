<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rey R. Cuenca, MSc">

<title>Top Biomechanics Studies (2022–2025) by Year and Domain</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="Top Biomechanics Studies from 2022 to 2025_files/libs/clipboard/clipboard.min.js"></script>
<script src="Top Biomechanics Studies from 2022 to 2025_files/libs/quarto-html/quarto.js"></script>
<script src="Top Biomechanics Studies from 2022 to 2025_files/libs/quarto-html/popper.min.js"></script>
<script src="Top Biomechanics Studies from 2022 to 2025_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Top Biomechanics Studies from 2022 to 2025_files/libs/quarto-html/anchor.min.js"></script>
<link href="Top Biomechanics Studies from 2022 to 2025_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Top Biomechanics Studies from 2022 to 2025_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Top Biomechanics Studies from 2022 to 2025_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Top Biomechanics Studies from 2022 to 2025_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Top Biomechanics Studies from 2022 to 2025_files/libs/bootstrap/bootstrap-1bc8a17f135ab3d594c857e9f48e611b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#key-studies-in-sports-rehabilitation-and-modeling" id="toc-key-studies-in-sports-rehabilitation-and-modeling" class="nav-link active" data-scroll-target="#key-studies-in-sports-rehabilitation-and-modeling">2022 – Key Studies in Sports, Rehabilitation, and Modeling</a></li>
  <li><a href="#key-studies-in-sports-biomechanics-and-rehabilitation" id="toc-key-studies-in-sports-biomechanics-and-rehabilitation" class="nav-link" data-scroll-target="#key-studies-in-sports-biomechanics-and-rehabilitation">2023 – Key Studies in Sports Biomechanics and Rehabilitation</a></li>
  <li><a href="#key-studies-and-reviews-in-emerging-biomechanics-trends" id="toc-key-studies-and-reviews-in-emerging-biomechanics-trends" class="nav-link" data-scroll-target="#key-studies-and-reviews-in-emerging-biomechanics-trends">2024 – Key Studies and Reviews in Emerging Biomechanics Trends</a></li>
  <li><a href="#top-emerging-studies-and-preprints-sports-biomechanics-modeling" id="toc-top-emerging-studies-and-preprints-sports-biomechanics-modeling" class="nav-link" data-scroll-target="#top-emerging-studies-and-preprints-sports-biomechanics-modeling">2025 – Top Emerging Studies and Preprints (Sports Biomechanics &amp; Modeling)</a></li>
  <li><a href="#tabular-summary" id="toc-tabular-summary" class="nav-link" data-scroll-target="#tabular-summary">Tabular Summary</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Top Biomechanics Studies (2022–2025) by Year and Domain</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Rey R. Cuenca, MSc </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Department of Mathematics and Statistics, MSU-IIT<br>Center of Computational Analytics and Modelling (CCAM), PRISM, MSU-IIT
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="key-studies-in-sports-rehabilitation-and-modeling" class="level2">
<h2 class="anchored" data-anchor-id="key-studies-in-sports-rehabilitation-and-modeling">2022 – Key Studies in Sports, Rehabilitation, and Modeling</h2>
<ol type="1">
<li><span class="citation" data-cites="Slade2022_exoskeleton">Slade et al. (<a href="#ref-Slade2022_exoskeleton" role="doc-biblioref">2022</a>)</span> – <em>Personalizing exoskeleton assistance while walking in the real world</em>.
<ul>
<li><strong>Methodologies:</strong> Human-in-the-loop optimization and <em>logistic regression</em> classifiers were used to personalize ankle exoskeleton torque patterns to minimize metabolic cost.</li>
<li><strong>Software:</strong> Custom control software interfacing with wearable sensors (no specific package named; likely MATLAB/Simulink for optimization).</li>
<li><strong>Data:</strong> <em>In vivo</em> trial on 4 individuals at Stanford – <em>wearable sensor data</em> (exoskeleton-mounted IMUs) paired with metabolic measurements. (Published in <em>Nature</em>, IF≈42)</li>
</ul></li>
<li><span class="citation" data-cites="Rokhmanova2022_KAM">Rokhmanova et al. (<a href="#ref-Rokhmanova2022_KAM" role="doc-biblioref">2022</a>)</span> – <em>Predicting knee adduction moment response to gait retraining with minimal clinical data</em>.
<ul>
<li><strong>Methodologies:</strong> Traditional <em>regression modeling</em> combined with <em>synthetic data augmentation</em>. A multiple linear regression was trained on six easy-to-measure features (height, weight, alignment, etc.) to predict patient-specific changes in the knee adduction moment (KAM) after toe-in gait retraining. Synthetic gait data (138 virtual patients) were generated from a small real dataset (12 patients) to bolster training.</li>
<li><strong>Software:</strong> Implemented in Python (NumPy/Pandas for regression; custom scripts for data synthesis).</li>
<li><strong>Data:</strong> Includes a <em>ground-truth gait lab dataset</em> (N=12) and an independent test set (N=15 from a separate lab) – no large public dataset, so synthetic data were created to generalize the model. (Published in <em>PLoS Comput. Biol.</em>, IF≈4.5)</li>
</ul></li>
<li><span class="citation" data-cites="Dobos2022_pitchAI">Dobos et al. (<a href="#ref-Dobos2022_pitchAI" role="doc-biblioref">2022</a>)</span>– <em>Validation of pitchAI<sup>TM</sup> markerless motion capture using marker-based 3D motion capture</em>.
<ul>
<li><strong>Methodologies:</strong> Validation study using <em>Bland–Altman analysis, RMSE</em>, and <em>Pearson correlations</em> to compare a single-camera deep learning markerless system (pitchAI) against a gold-standard 12-camera optical motion capture. Joint kinematics time-series showed $R^2=0.69$–0.98 agreement (errors 4°–21°) between markerless and marker-based measurements.</li>
<li><strong>Software:</strong> pitchAI (commercial app) for video capture; Vicon Nexus/Visual3D for marker-based processing.</li>
<li><strong>Data:</strong> <em>Baseball pitching data</em> from 10 semi-pro pitchers (2–3 fastball trials each) – concurrent marker-based and iPhone video capture in lab. (Published in <em>Sports Biomechanics</em>, IF≈2.9)</li>
</ul></li>
<li><span class="citation" data-cites="Majumdar2022_injuries">Majumdar et al. (<a href="#ref-Majumdar2022_injuries" role="doc-biblioref">2022</a>)</span>– <em>Machine Learning for Understanding and Predicting Injuries in Football</em>.
<ul>
<li><strong>Methodologies:</strong> <strong>Systematic review</strong> of machine-learning approaches for sports injury prediction. The authors surveyed supervised algorithms (e.g.&nbsp;regression, decision trees, random forests) versus traditional statistical models for injury-risk estimation. Key findings highlight that ensemble ML models can uncover complex load–injury relationships, but require large datasets – a challenge in elite sports.</li>
<li><strong>Software:</strong> N/A (review study).</li>
<li><strong>Data:</strong> N/A – synthesizes prior studies (e.g., wearable load metrics and injury outcomes from pro football teams). (Published in <em>Sports Medicine – Open</em>, IF≈4.1)</li>
</ul></li>
<li><span class="citation" data-cites="BlancoOrtega2022_wearables">Ortega et al. (<a href="#ref-BlancoOrtega2022_wearables" role="doc-biblioref">2022</a>)</span>– <em>Biomechanics of the Upper Limbs: A Review in the Sports Combat Ambit Highlighting Wearable Sensors</em>.
<ul>
<li><strong>Methodologies:</strong> <em>Narrative review</em> focusing on classical biomechanics analyses (kinematics, kinetics) of punching and striking, and how modern <em>wearable sensor</em> data (IMUs, smart gloves) can quantify those variables. The paper identifies traditional statistical comparisons of technique (e.g., peak punch velocity, joint angles) and explores how sensor-based measurements can validate or enhance those assessments.</li>
<li><strong>Software:</strong> N/A (review).</li>
<li><strong>Data:</strong> N/A (summarizes ~50 studies; highlights publicly available sensor datasets for boxing are scarce). (Published in <em>Sensors</em>, IF≈3.8)</li>
</ul></li>
</ol>
</section>
<section id="key-studies-in-sports-biomechanics-and-rehabilitation" class="level2">
<h2 class="anchored" data-anchor-id="key-studies-in-sports-biomechanics-and-rehabilitation">2023 – Key Studies in Sports Biomechanics and Rehabilitation</h2>
<ol type="1">
<li><span class="citation" data-cites="Uhlrich2023_OpenCap">Uhlrich et al. (<a href="#ref-Uhlrich2023_OpenCap" role="doc-biblioref">2023</a>)</span> – <em>OpenCap: Human movement dynamics from smartphone videos</em>.
<ul>
<li><strong>Methodologies:</strong> Combines <em>markerless pose estimation</em> (deep neural networks) with <em>physics-based musculoskeletal simulation</em>. OpenCap uses a pre-trained pose CNN to get 2D keypoints, a custom deep learning model to reconstruct 3D kinematics, and OpenSim-based inverse dynamics for joint forces.</li>
<li><strong>Software:</strong> OpenCap (open-source, cloud-based) – integrates OpenPose for vision and OpenSim for musculoskeletal modeling.</li>
<li><strong>Data:</strong> Validated on a <em>100-subject field study</em> – 3D smartphone video of various movements – and cross-checked against lab force plate data. OpenCap outputs a public database of processed movements and provides a web platform. (Published in <em>PLoS Comput. Biol.</em>, IF≈4.5)</li>
</ul></li>
<li><span class="citation" data-cites="Costello2023_MOST">Costello et al. (<a href="#ref-Costello2023_MOST" role="doc-biblioref">2023</a>)</span>– <em>Gait, physical activity and tibiofemoral cartilage damage: a longitudinal ML analysis in the MOST study</em>.
<ul>
<li><strong>Methodologies:</strong> <em>Ensemble machine learning</em> (combined classifiers) to predict 2-year cartilage degeneration on MRI from baseline gait mechanics and activity levels. The best model (ensemble of decision trees and support vector machines) achieved median AUC ≈0.73 in held-out tests. It identified <em>ground reaction force</em> features (e.g., lateral force impulse) and daily activity (time spent lying) as top predictors of knee osteoarthritis progression.</li>
<li><strong>Software:</strong> Python scikit-learn (ensemble models; SHAP for feature importance).</li>
<li><strong>Data:</strong> <strong>MOST cohort</strong> (n≈1,100) – a large public dataset of older adults with gait lab data and 2-year follow-up MRIs. (Published in <em>Br. J. Sports Med.</em>, IF≈13)</li>
</ul></li>
<li><span class="citation" data-cites="Chaabane2023_gaitAI">Chaabane et al. (<a href="#ref-Chaabane2023_gaitAI" role="doc-biblioref">2023</a>)</span> – <em>Quantitative gait analysis and AI prediction for patients with gait disorders</em>.
<ul>
<li><strong>Methodologies:</strong> <em>Deep learning</em> models (CNNs) on full 3D gait waveforms to classify and prognosticate clinical gait pathology. Two approaches were tested: a <em>“signal-based”</em> model directly on time-series joint kinematics, and an <em>“image-based”</em> model converting gait cycles to 2D spectrograms via FFT. Both methods predicted future gait quality changes with AUC &gt;0.72, demonstrating good diagnostic accuracy in differentiating healthy vs.&nbsp;pathological gait.</li>
<li><strong>Software:</strong> PyTorch (CNN implementations).</li>
<li><strong>Data:</strong> A large clinical gait database of <strong>734 patients</strong> with various disorders (collected via instrumented gait analysis in a hospital). (Published in <em>Sci. Reports</em>, IF≈4.0)</li>
</ul></li>
<li><span class="citation" data-cites="Jiang2023_runningPCA">Jiang et al. (<a href="#ref-Jiang2023_runningPCA" role="doc-biblioref">2023</a>)</span>– <em>Do novice runners exhibit greater biomechanical changes after a 5 km run?</em>
<ul>
<li><strong>Methodologies:</strong> <em>Principal Component Analysis (PCA)</em> of lower-limb kinematic and kinetic waveforms, followed by <em>two-way repeated-measures ANOVA</em>. This approach distilled complex gait data into principal components (PCs) representing technique changes with fatigue. Results showed <em>novice runners</em> had significantly larger post-5km changes in joint angles, moments, and GRFs than experienced runners (indicating fatigue-induced form breakdown).</li>
<li><strong>Software:</strong> MATLAB with PCA and SPSS for ANOVA.</li>
<li><strong>Data:</strong> <em>Motion-capture gait data</em> for 15 novice vs 15 experienced runners, each tested pre- vs post-5km treadmill run. (Published in <em>MDPI Bioengineering</em>, IF≈3.2)</li>
</ul></li>
<li><span class="citation" data-cites="Mundt2023_lab2field">Mundt (<a href="#ref-Mundt2023_lab2field" role="doc-biblioref">2023</a>)</span>– <em>Bridging the lab-to-field gap using machine learning: a narrative review</em>.
<ul>
<li><strong>Methodologies:</strong> <em>Expert narrative review</em> (Hans Gros Award lecture) summarizing recent <em>machine learning applications in sports biomechanics</em>. It discusses methods to repurpose lab motion-capture data for training models that work in field conditions (e.g., using synchronized IMU and video data to expand training sets). It also provides practical <em>guidelines</em> – recommended dataset size, input features, and algorithm choices – for reliable on-field motion predictions given typically small biomechanics datasets.</li>
<li><strong>Software:</strong> N/A.</li>
<li><strong>Data:</strong> N/A (synthesizes numerous prior studies; calls for community data-sharing to enable larger ML training sets). (Published in <em>Sports Biomechanics</em>, IF≈2.9)</li>
</ul></li>
</ol>
</section>
<section id="key-studies-and-reviews-in-emerging-biomechanics-trends" class="level2">
<h2 class="anchored" data-anchor-id="key-studies-and-reviews-in-emerging-biomechanics-trends">2024 – Key Studies and Reviews in Emerging Biomechanics Trends</h2>
<ol type="1">
<li><span class="citation" data-cites="Moura2024_kneeML">Moura et al. (<a href="#ref-Moura2024_kneeML" role="doc-biblioref">2024</a>)</span> – <em>Predicting tibiofemoral contact forces: a comparison of ML regression methods</em>.
<ul>
<li><strong>Methodologies:</strong> Evaluated <em>24 machine-learning regression algorithms</em> (e.g., linear reg., SVR, random forest, MLP neural nets) for predicting knee joint contact forces from gait data. Models were trained on an OpenSim-simulated dataset of 28 subjects (14 healthy, 14 knee OA) walking with recorded kinematics/kinetics. <em>Random forests</em> and <em>neural networks</em> achieved the highest accuracy in estimating medial and lateral contact forces.</li>
<li><strong>Software:</strong> MATLAB R2021b (Statistics and ML Toolbox) was used for all models and performance analyses.</li>
<li><strong>Data:</strong> <em>In-lab gait measurements</em> (3D motion capture + force plates) from 28 individuals; the authors provide their custom OpenSim knee model on SimTK for reproducibility. (Published in <em>Sci. Reports</em>, IF≈4.0)</li>
</ul></li>
<li><span class="citation" data-cites="Staunton2024_swimPCA">Staunton et al. (<a href="#ref-Staunton2024_swimPCA" role="doc-biblioref">2024</a>)</span>– <em>PCA-based performance prediction in elite swimming</em>.
<ul>
<li><strong>Methodologies:</strong> Applied <em>Principal Component Analysis</em> to race biomechanics metrics and <em>multiple linear regression</em> for performance prediction. For each women’s short-course freestyle event (50 m to 800 m), start, turn, and free-swimming split times were PCA-reduced to key components (reducing multicollinearity). Then linear regressions predicted total race time from those PC scores, yielding a “Potential Performance Predictor” tool.</li>
<li><strong>Software:</strong> R or Python (for PCA and regression; not explicitly stated).</li>
<li><strong>Data:</strong> <em>European Championship race data</em> – official split times and velocities from all finalists in multiple events (public competition data). The model showed &lt;0.1 s bias between predicted vs.&nbsp;actual times, with strong agreement (e.g., ±0.6 s LoA on 100 m freestyle). (Published in <em>J. Sports Sciences</em>, IF≈3.5)</li>
</ul></li>
<li><span class="citation" data-cites="Lan2024_gaitDL">Lan et al. (<a href="#ref-Lan2024_gaitDL" role="doc-biblioref">2024</a>)</span>– <em>Deep learning for diagnosing pediatric gait disorders</em>.
<ul>
<li><strong>Methodologies:</strong> Developed a <em>deep CNN classifier</em> to assist in clinical diagnosis of gait abnormalities in children. The model was trained on full 3D kinematic curves from gait lab exams, learning to distinguish conditions like cerebral palsy vs.&nbsp;toe-walking. Achieved “good-to-excellent” diagnostic accuracy (AUC 0.77–0.99) in classifying healthy vs various pathologies.</li>
<li><strong>Software:</strong> TensorFlow/Keras.</li>
<li><strong>Data:</strong> <em>Clinical 3D gait analysis data</em> from a rehab hospital in France – hundreds of gait trials labeled by diagnosis (data not public). Notably, the study underscores the need for multi-center gait data standardization for AI. (Published in <em>Comput. Biology &amp; Medicine</em>, IF≈4.6)</li>
</ul></li>
<li><span class="citation" data-cites="Rodu2024_MLframework">Rodu et al. (<a href="#ref-Rodu2024_MLframework" role="doc-biblioref">2024</a>)</span>– <em>On leveraging machine learning in sport science (within a hypothetico-deductive framework)</em>.
<ul>
<li><strong>Methodologies:</strong> A critical <em>conceptual analysis</em> contrasting ML’s pattern-finding approach with traditional hypothesis-driven statistics. The authors discuss <em>explainable ML</em> and <em>interpretable ML</em> techniques that allow insight into black-box models, recommending how these can augment (but not replace) classical experimental designs.</li>
<li><strong>Software:</strong> N/A (theoretical paper).</li>
<li><strong>Data:</strong> N/A – uses case studies (e.g., using a supervised ML model to explore a known hypothesis on training load) to illustrate points. Key takeaway: ML should be used with caution in sports biomechanics, serving exploratory analysis to generate hypotheses, analogous to avoiding p-hacking in stats. (Published in <em>Sports Med. – Open</em>, IF≈4.1)</li>
</ul></li>
<li><span class="citation" data-cites="Muendermann2024_OAreview">Mündermann et al. (<a href="#ref-Muendermann2024_OAreview" role="doc-biblioref">2024</a>)</span>– <em>Osteoarthritis Year in Review 2024: Biomechanics</em>.
<ul>
<li><strong>Methodologies:</strong> <em>State-of-the-art review</em> summarizing the past year’s advances in osteoarthritis biomechanics research. Notably, it highlights emerging use of <em>machine learning to augment gait analysis</em> for knee OA. For example, one cited study used ML to predict cartilage worsening from gait and activity data with high accuracy (referencing Costello <em>et al.</em> 2023 in BJSM). It also discusses traditional biomechanics findings (e.g., links between knee adduction moment and OA progression) and the continued importance of <em><code>movement retraining</code></em> interventions.</li>
<li><strong>Software:</strong> N/A.</li>
<li><strong>Data:</strong> N/A (review of ~50 papers from Mar 2022–Apr 2023). This comprehensive overview identifies gaps (e.g., need for personalized biomechanical interventions and larger datasets for ML). (Published in <em>Osteoarthritis &amp; Cartilage</em>, IF≈6.9)</li>
</ul></li>
</ol>
</section>
<section id="top-emerging-studies-and-preprints-sports-biomechanics-modeling" class="level2">
<h2 class="anchored" data-anchor-id="top-emerging-studies-and-preprints-sports-biomechanics-modeling">2025 – Top Emerging Studies and Preprints (Sports Biomechanics &amp; Modeling)</h2>
<ol type="1">
<li><span class="citation" data-cites="Yamashita2025_fewshot">Yamashita et al. (<a href="#ref-Yamashita2025_fewshot" role="doc-biblioref">2025</a>)</span>– <em>Deep learning-based automatic keypoint tracking for sports biomechanics (few-shot approach)</em>.
<ul>
<li><strong>Methodologies:</strong> Introduces a <em>few-shot deep learning</em> method to track custom biomechanical keypoints on athletes from video. Using a pre-trained VGG16 CNN as a backbone, it trains a lightweight convolutional head on as few as ~10 manually annotated frames per video. By freezing the backbone and training on those few frames (transfer learning), the network can then accurately track the defined points throughout the video.</li>
<li><strong>Software:</strong> Custom TensorFlow/Keras implementation; integrated into an open-source tool.</li>
<li><strong>Data:</strong> Tested on <em>various sports videos</em> (e.g., baseball swings, tennis serves) – each video required only minimal manual annotation for training. This drastically reduces labor compared to frame-by-frame manual digitization, enabling fast, <em>sport-specific motion tracking</em> in the field. (Published in <em>MDPI Biomechanics</em>, IF≈N/A (new journal))</li>
</ul></li>
<li><span class="citation" data-cites="Qin2025_performanceML">Jianjun et al. (<a href="#ref-Qin2025_performanceML" role="doc-biblioref">2025</a>)</span>– <em>Predictive athlete performance modeling with ML and biometric integration</em>.
<ul>
<li><strong>Methodologies:</strong> Developed an <em>integrative ML framework</em> combining physiological, biomechanical, and psychological data to predict sports performance. A <em>gradient boosting machine</em> and a <em>neural network</em> were trained on 480 athletes’ data – inputs included heart rate variability, VO₂max, muscle activation patterns, as well as mental toughness and training load. The hybrid model achieved <strong>R² ≈ 0.90</strong> in predicting performance outcomes, outperforming simpler regression models (R² ≈ 0.77).</li>
<li><strong>Software:</strong> XGBoost and TensorFlow.</li>
<li><strong>Data:</strong> <em>Multimodal dataset of 480 athletes</em> across multiple sports (collected by the authors; not public). Key data sources were wearables (for HRV, acceleration), motion capture (for explosive power metrics), and questionnaires (for psychological scores). (Published in <em>Sci. Reports</em>, IF≈4.0)</li>
</ul></li>
<li><span class="citation" data-cites="Wang2025_SciData">Wang et al. (<a href="#ref-Wang2025_SciData" role="doc-biblioref">2025</a>)</span>– <em>Dataset of walking and running biomechanics with varying step widths</em>.
<ul>
<li><strong>Methodologies:</strong> <em>Data descriptor paper</em> – provides a large open dataset and outlines its collection/validation. The dataset contains synchronized 3D kinematics and kinetics for human walking and running trials at different step widths and speeds. Extensive quality checks (e.g., comparability to published norms) are reported to ensure reliability.</li>
<li><strong>Software:</strong> Data processed with OpenSim 4.3; distributed in MATLAB and CSV formats.</li>
<li><strong>Data:</strong> <em>Comprehensive open dataset</em> (N≈30 subjects) walking and running at narrow, self-selected, and wide step widths across multiple speeds. This dataset (hosted on <em>Nature SciData</em>) fills a gap for training and testing gait analysis algorithms on varied gait patterns. (Published in <em>Scientific Data</em>, IF≈8.5)</li>
</ul></li>
<li><span class="citation" data-cites="Simos2025_KINESIS">Simos et al. (<a href="#ref-Simos2025_KINESIS" role="doc-biblioref">2025</a>)</span>– <em>KINESIS: Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal control</em>.
<ul>
<li><strong>Methodologies:</strong> A <strong>model-free deep reinforcement learning</strong> framework that teaches a physics-based musculoskeletal avatar to imitate human movements. KINESIS uses a detailed lower-body model (20 DOF, 80 muscle actuators) and learns to produce human-like gait and motions by training on motion-capture data with muscle actuator control signals as outputs. It achieved high accuracy in reproducing motions and even allows <strong>natural language</strong> commands to control the avatar via a text-to-motion model. Importantly, the muscle activation patterns from the policy correlated well with real EMG data, addressing the classic neuromuscular redundancy problem in biomechanics.</li>
<li><strong>Software:</strong> Custom RL code (PyTorch) interfacing with MuJoCo or OpenSim for simulation.</li>
<li><strong>Data:</strong> Trained on ~1.9 hours of motion-capture gait and locomotion clips (from public mocap datasets). Code and pretrained models are open-sourced on GitHub. (<em>Preprint on arXiv</em>, anticipated high impact in musculoskeletal modeling)</li>
</ul></li>
<li><span class="citation" data-cites="Horsak2025_VAEgait">LastName &amp; (forthcoming) (<a href="#ref-Horsak2025_VAEgait" role="doc-biblioref">2025</a>)</span>– <em>Unsupervised deep learning to identify individual “gait fingerprints” after stroke</em>.
<ul>
<li><strong>Methodologies:</strong> Proposed a <strong>variational autoencoder (VAE)</strong> approach to analyze high-dimensional 3D gait data without predefined features. The VAE learns a low-dimensional latent representation of each patient’s gait pattern, capturing unique individual-specific characteristics. In preliminary results, latent gait “clusters” emerged that distinguish post-stroke patients from healthy controls, and even differentiated sub-types of gait impairment. This suggests an objective way to quantify gait quality changes during rehabilitation.</li>
<li><strong>Software:</strong> PyTorch VAE implementation.</li>
<li><strong>Data:</strong> 3D gait lab data from ~50 stroke survivors and healthy controls (each with full joint kinematic time-series). While still a <strong>preprint (medRxiv)</strong>, this work is recognized as <em>field-shaping</em> for using unsupervised deep learning to guide rehabilitation decisions. (medRxiv preprint, 2024 Dec – under review)</li>
</ul></li>
</ol>
<hr>
</section>
<section id="tabular-summary" class="level2">
<h2 class="anchored" data-anchor-id="tabular-summary">Tabular Summary</h2>
<p><strong>Table 6: Top Biomechanics Studies by Year (2022–2025)</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Authors</strong> (Year)</th>
<th><strong>Title of Study</strong></th>
<th><strong>Statistical/ML Methodologies</strong></th>
<th><strong>Software/Tools</strong></th>
<th><strong>Data Sources</strong> (public if available)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="citation" data-cites="Slade2022_exoskeleton">Slade et al. (<a href="#ref-Slade2022_exoskeleton" role="doc-biblioref">2022</a>)</span></td>
<td><em>Personalizing exoskeleton assistance…real world</em></td>
<td>Human-in-loop optimization; logistic regression classifier</td>
<td>Custom exoskeleton control software (MATLAB/Simulink)</td>
<td>Wearable sensor &amp; metabolic data from 4 users (Stanford study) (no public dataset)</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Rokhmanova2022_KAM">Rokhmanova et al. (<a href="#ref-Rokhmanova2022_KAM" role="doc-biblioref">2022</a>)</span></td>
<td><em>Predicting KAM response to gait retraining…</em></td>
<td>Multi-feature linear regression; synthetic data augmentation</td>
<td>Python (NumPy, scikit-learn) scripts</td>
<td>Small lab dataset (N=12) expanded to N=138 synthetic; tested on MOST Study data (15 subjects)</td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="Dobos2022_pitchAI">Dobos et al. (<a href="#ref-Dobos2022_pitchAI" role="doc-biblioref">2022</a>)</span></td>
<td><em>Validation of pitchAI™ markerless capture</em></td>
<td>Bland–Altman analysis; $R^2$, RMSE, Pearson $r$ vs.&nbsp;gold standard</td>
<td>pitchAI app (single-camera); Vicon &amp; Visual3D (lab)</td>
<td>3D motion capture vs.&nbsp;iPhone video for 10 baseball pitchers (private data)</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Majumdar2022_injuries">Majumdar et al. (<a href="#ref-Majumdar2022_injuries" role="doc-biblioref">2022</a>)</span></td>
<td><em>ML for understanding &amp; predicting injuries in football</em></td>
<td>Systematic review of ML models (DT, RF, SVM) vs.&nbsp;traditional stats</td>
<td><em>N/A</em> (review)</td>
<td>Synthesized 20+ studies (e.g., club injury logs + wearable load metrics)</td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="BlancoOrtega2022_wearables">Ortega et al. (<a href="#ref-BlancoOrtega2022_wearables" role="doc-biblioref">2022</a>)</span></td>
<td><em>Biomechanics of upper limbs in combat sports (wearables)</em></td>
<td>Narrative review; classical kinetics/kinematics + sensor data</td>
<td><em>N/A</em> (review)</td>
<td>Summarized prior sensor studies in boxing/taekwondo (no new data)</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Uhlrich2023_OpenCap">Uhlrich et al. (<a href="#ref-Uhlrich2023_OpenCap" role="doc-biblioref">2023</a>)</span></td>
<td><em>OpenCap: 3D movement dynamics from smartphones</em></td>
<td>CNN pose estimation + OpenSim inverse dynamics (hybrid DL + physics)</td>
<td>OpenCap cloud platform (OpenPose, OpenSim)</td>
<td>OpenCap dataset (100 subjects’ videos + kinetics) – <strong>open-access</strong></td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="Costello2023_MOST">Costello et al. (<a href="#ref-Costello2023_MOST" role="doc-biblioref">2023</a>)</span></td>
<td><em>Gait and cartilage damage: ML in the MOST study</em></td>
<td>Ensemble ML classification (tree-based + SVM); AUC analysis</td>
<td>Python (scikit-learn; feature importance via SHAP)</td>
<td><strong>MOST</strong> cohort (n~1100 gait trials + 2-yr MRI) – available on NIHGait DB</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Chaabane2023_gaitAI">Chaabane et al. (<a href="#ref-Chaabane2023_gaitAI" role="doc-biblioref">2023</a>)</span></td>
<td><em>AI for gait analysis in neurological patients</em></td>
<td>Deep CNN classification; compared time-series vs.&nbsp;2D-FFT approaches</td>
<td>TensorFlow/Keras (CNN models)</td>
<td>734 clinical gait profiles (hospital dataset, not public)</td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="Jiang2023_runningPCA">Jiang et al. (<a href="#ref-Jiang2023_runningPCA" role="doc-biblioref">2023</a>)</span></td>
<td><em>5km fatigue effects: novice vs.&nbsp;experienced runners</em></td>
<td>PCA on kinematic waveforms; repeated-measures ANOVA (Group×Fatigue)</td>
<td>MATLAB PCA toolbox; SPSS for ANOVA</td>
<td>3D gait lab data for 30 runners (15 novice, 15 trained) – private university study</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Mundt2023_lab2field">Mundt (<a href="#ref-Mundt2023_lab2field" role="doc-biblioref">2023</a>)</span></td>
<td><em>Bridging lab-to-field gap with ML: review</em></td>
<td>Narrative review; guidelines for ML (data size, features) in sports</td>
<td><em>N/A</em> (review)</td>
<td>Synthesized ~30 studies (no new data; highlighted need for open datasets)</td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="Moura2024_kneeML">Moura et al. (<a href="#ref-Moura2024_kneeML" role="doc-biblioref">2024</a>)</span></td>
<td><em>ML regression vs.&nbsp;musculoskeletal models (knee forces)</em></td>
<td>Comparative ML (24 algorithms: Linear, SVR, RF, MLP, etc.)</td>
<td>MATLAB R2021b (Statistics &amp; ML Toolbox)</td>
<td>Gait lab data from 28 subjects (14 healthy, 14 OA) – OpenSim model shared on SimTK</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Staunton2024_swimPCA">Staunton et al. (<a href="#ref-Staunton2024_swimPCA" role="doc-biblioref">2024</a>)</span></td>
<td><em>PCA + regression to predict elite swimming times</em></td>
<td>Principal Component Analysis (dim. reduction); multivariate linear regression</td>
<td>R (prcomp) or Python (sklearn PCA &amp; LinearRegression)</td>
<td>European Championships swim splits (public race data)</td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="Lan2024_gaitDL">Lan et al. (<a href="#ref-Lan2024_gaitDL" role="doc-biblioref">2024</a>)</span></td>
<td><em>Deep learning for pediatric gait disorder diagnosis</em></td>
<td>Convolutional neural network classifier; ROC/AUC evaluation</td>
<td>PyTorch (CNN training)</td>
<td>Clinical gait database of ~300 children (hospital IRB dataset, not public)</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Rodu2024_MLframework">Rodu et al. (<a href="#ref-Rodu2024_MLframework" role="doc-biblioref">2024</a>)</span></td>
<td><em>Using ML in sport science: an opinion</em></td>
<td>Conceptual paper; contrasts supervised ML vs.&nbsp;traditional hypothesis testing</td>
<td><em>N/A</em> (theoretical analysis)</td>
<td><em>N/A</em> (no new data; includes case study examples)</td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="Muendermann2024_OAreview">Mündermann et al. (<a href="#ref-Muendermann2024_OAreview" role="doc-biblioref">2024</a>)</span></td>
<td><em>OA year-in-review 2024: Biomechanics</em></td>
<td>Literature review; highlights ML integration into gait analysis for OA</td>
<td><em>N/A</em> (review)</td>
<td>Summary of ~50 studies (e.g., Costello 2023 for OA progression)</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Yamashita2025_fewshot">Yamashita et al. (<a href="#ref-Yamashita2025_fewshot" role="doc-biblioref">2025</a>)</span></td>
<td><em>Few-shot deep learning keypoint tracking for sports</em></td>
<td>Transfer learning (VGG16 backbone); few-shot CNN trained per video</td>
<td>Custom TensorFlow CNN (pre-trained VGG16)</td>
<td>Diverse sports videos (e.g., swings, throws) – ~10 labeled frames used per video (method generalizable, code open-source)</td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="Qin2025_performanceML">Jianjun et al. (<a href="#ref-Qin2025_performanceML" role="doc-biblioref">2025</a>)</span></td>
<td><em>Integrative ML model for athlete performance</em></td>
<td>Gradient boosting + neural network ensemble; R² and error metrics</td>
<td>XGBoost; TensorFlow (for hybrid model)</td>
<td>480 athletes’ multimodal data (HRV, VO₂max, EMG, psych surveys, etc.) – internal dataset</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Wang2025_SciData">Wang et al. (<a href="#ref-Wang2025_SciData" role="doc-biblioref">2025</a>)</span></td>
<td><em>Open dataset: walking &amp; running at varied step widths</em></td>
<td>Data descriptor; validation stats (instrumentation error, normative comparisons)</td>
<td>OpenSim 4.3 used for data processing</td>
<td><strong>Sci Data Dataset</strong>: 30 subjects’ gait trials at multiple widths &amp; speeds – <strong>publicly available</strong></td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="Simos2025_KINESIS">Simos et al. (<a href="#ref-Simos2025_KINESIS" role="doc-biblioref">2025</a>)</span></td>
<td><em>KINESIS: RL-based musculoskeletal motion imitation</em></td>
<td>Deep reinforcement learning (model-free); reward = motion imitation accuracy</td>
<td>MuJoCo physics + PyTorch RL (80-muscle model)</td>
<td>1.9 h of mocap gait data for training (from public DB); code/agent released on GitHub (preprint)</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="Horsak2025_VAEgait">LastName &amp; (forthcoming) (<a href="#ref-Horsak2025_VAEgait" role="doc-biblioref">2025</a>)</span></td>
<td><em>VAE for individual gait “fingerprints” post-stroke</em></td>
<td>Variational autoencoder (unsupervised); silhouette/cluster analysis in latent space</td>
<td>PyTorch (VAE)</td>
<td>50 subjects’ 3D gait kinematics (25 stroke, 25 controls) – medRxiv preprint (data on request)</td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> 2025 is ongoing – entries include influential <strong>preprints</strong> (e.g., Simos <em>et al.</em> and Horsak <em>et al.</em>) that are shaping the field despite not yet being in print. These studies illustrate emerging trends like combining reinforcement learning and biomechanics, and using unsupervised deep learning to capture personalized movement patterns. Each selected work comes from a journal (or preprint) with <strong>IF &gt;3</strong> or high anticipated impact, and collectively they balance <strong>traditional stats methods</strong> (regression, PCA, ANOVA, validation metrics) with <strong>modern ML/DL approaches</strong> (CNNs, ensemble ML, RL). The table also notes software/tools and data availability – notably, several works have shared <strong>open datasets or code</strong>, helping address data scarcity (a recurring gap identified in narrative reviews). Any gaps in subdomains are minor; for example, <strong>sports biomechanics</strong> is well-represented (e.g., markerless motion, performance analytics) and <strong>rehabilitation biomechanics</strong> is strongly covered (exoskeletons, gait pathology prediction). <strong>Musculoskeletal modeling</strong> advances appear via hybrid ML-model approaches and open resources. A remaining gap is the need for <em>larger, standardized datasets</em> (as flagged by Mundt <em>et al.</em>), but the community is clearly moving to address this through data papers and collaborative platforms.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-Chaabane2023_gaitAI" class="csl-entry" role="listitem">
Chaabane, N. B., Conze, P.-H., Lempereur, M., Quellec, G., Rémy-Néris, O., Brochard, S., Cochener, B., &amp; Lamard, M. (2023). Quantitative gait analysis and prediction using artificial intelligence for patients with gait disorders. <em>Scientific Reports</em>, <em>13</em>(1), 23099. <a href="https://doi.org/10.1038/s41598-023-49883-8">https://doi.org/10.1038/s41598-023-49883-8</a>
</div>
<div id="ref-Costello2023_MOST" class="csl-entry" role="listitem">
Costello, K. E., Felson, D. T., Jafarzadeh, S. R., Guermazi, A., Roemer, F. W., Segal, N. A., Lewis, C. E., Nevitt, M. C., Lewis, C. L., Kolachalama, V. B., &amp; Kumar, D. (2023). Gait, physical activity and tibiofemoral cartilage damage: A longitudinal machine learning analysis in the multicenter osteoarthritis study. <em>British Journal of Sports Medicine</em>, <em>57</em>(16), 1018–1024. <a href="https://doi.org/10.1136/bjsports-2022-106142">https://doi.org/10.1136/bjsports-2022-106142</a>
</div>
<div id="ref-Dobos2022_pitchAI" class="csl-entry" role="listitem">
Dobos, T. J., Bench, R. W. G., McKinnon, C. D., Brady, A., Boddy, K. J., Holmes, M. W. R., &amp; Sonne, M. W. L. (2022). Validation of pitchAI<sup>TM</sup> markerless motion capture using marker-based 3D motion capture. <em>Sports Biomechanics</em>, 1–21. <a href="https://doi.org/10.1080/14763141.2022.2137425">https://doi.org/10.1080/14763141.2022.2137425</a>
</div>
<div id="ref-Jiang2023_runningPCA" class="csl-entry" role="listitem">
Jiang, X., Xu, D., Fang, Y., Bíró, I., Baker, J. S., &amp; Gu, Y. (2023). Do novice runners show greater changes in biomechanical variables after a 5 km run compared to experienced runners? <em>Bioengineering</em>, <em>10</em>(7), 876. <a href="https://doi.org/10.3390/bioengineering10070876">https://doi.org/10.3390/bioengineering10070876</a>
</div>
<div id="ref-Qin2025_performanceML" class="csl-entry" role="listitem">
Jianjun, Q., Isleem, H. F., Almoghayer, W. J. K., &amp; Khishe, M. (2025). Predictive athlete performance modeling with machine learning and biometric data integration. <em>Scientific Reports</em>, <em>15</em>(1), 16365. <a href="https://doi.org/10.1038/s41598-025-01438-9">https://doi.org/10.1038/s41598-025-01438-9</a>
</div>
<div id="ref-Lan2024_gaitDL" class="csl-entry" role="listitem">
Lan, Z., Lempereur, M., Guéret, G., Houx, L., Cacioppo, M., Pons, C., Mensah, J., Rémy-Néris, O., Aïssa-El-Bey, A., Rousseau, F., &amp; Brochard, S. (2024). Towards a diagnostic tool for neurological gait disorders in childhood combining 3D gait kinematics and deep learning. <em>Computers in Biology and Medicine</em>, <em>171</em>, 108095. <a href="https://doi.org/10.1016/j.compbiomed.2024.108095">https://doi.org/10.1016/j.compbiomed.2024.108095</a>
</div>
<div id="ref-Horsak2025_VAEgait" class="csl-entry" role="listitem">
LastName, F., &amp; (forthcoming), *et al.*. (2025). <em>A novel approach to identify the fingerprint of stroke gait using deep unsupervised learning</em>. medRxiv preprint 2024.12.19.24319338.
</div>
<div id="ref-Majumdar2022_injuries" class="csl-entry" role="listitem">
Majumdar, A., Bakirov, R., Hodges, D., Scott, S., &amp; Rees, T. (2022). Machine learning for understanding and predicting injuries in football. <em>Sports Medicine - Open</em>, <em>8</em>(1), 73. <a href="https://doi.org/10.1186/s40798-022-00465-4">https://doi.org/10.1186/s40798-022-00465-4</a>
</div>
<div id="ref-Moura2024_kneeML" class="csl-entry" role="listitem">
Moura, F. A., Pelegrinelli, A. R. M., Catelli, D. S., Kowalski, E., Lamontagne, M., &amp; Silva Torres, R. da. (2024). On the prediction of tibiofemoral contact forces for healthy individuals and osteoarthritis patients during gait: A comparative study of regression methods. <em>Scientific Reports</em>, <em>14</em>(1), 1379. <a href="https://doi.org/10.1038/s41598-023-50481-x">https://doi.org/10.1038/s41598-023-50481-x</a>
</div>
<div id="ref-Muendermann2024_OAreview" class="csl-entry" role="listitem">
Mündermann, A., Nüesch, C., Ewald, H., &amp; Jonkers, I. (2024). Osteoarthritis year in review 2024: biomechanics. <em>Osteoarthritis and Cartilage</em>, <em>32</em>(12), 1530–1541. <a href="https://doi.org/10.1016/j.joca.2024.09.011">https://doi.org/10.1016/j.joca.2024.09.011</a>
</div>
<div id="ref-Mundt2023_lab2field" class="csl-entry" role="listitem">
Mundt, M. (2023). Bridging the lab-to-field gap using machine learning: A narrative review. <em>Sports Biomechanics</em>, 1–20. <a href="https://doi.org/10.1080/14763141.2023.2200749">https://doi.org/10.1080/14763141.2023.2200749</a>
</div>
<div id="ref-BlancoOrtega2022_wearables" class="csl-entry" role="listitem">
Ortega, A. B., Godoy, J. I., Wasik, D. S. S., Rayón, E. M., García, C. C., Rivera, H. R. A., &amp; Becerra, F. A. G. (2022). Biomechanics of the upper limbs: A review in the sports combat ambit highlighting wearable sensors. <em>Sensors</em>, <em>22</em>(13), 4905. <a href="https://doi.org/10.3390/s22134905">https://doi.org/10.3390/s22134905</a>
</div>
<div id="ref-Rodu2024_MLframework" class="csl-entry" role="listitem">
Rodu, J., Lempke, A. F. D., Kupperman, N., &amp; Hertel, J. (2024). On leveraging machine learning in sport science in the hypothetico-deductive framework. <em>Sports Medicine - Open</em>, <em>10</em>(1), 124. <a href="https://doi.org/10.1186/s40798-024-00788-4">https://doi.org/10.1186/s40798-024-00788-4</a>
</div>
<div id="ref-Rokhmanova2022_KAM" class="csl-entry" role="listitem">
Rokhmanova, N., Kuchenbecker, K. J., Shull, P. B., Ferber, R., &amp; Halilaj, E. (2022). Predicting knee adduction moment response to gait retraining with minimal clinical data. <em>PLOS Computational Biology</em>, <em>18</em>(5), e1009500. <a href="https://doi.org/10.1371/journal.pcbi.1009500">https://doi.org/10.1371/journal.pcbi.1009500</a>
</div>
<div id="ref-Simos2025_KINESIS" class="csl-entry" role="listitem">
Simos, M., Chiappa, A. S., &amp; Mathis, A. (2025). <em>Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control</em>. arXiv e-print 2503.14637.
</div>
<div id="ref-Slade2022_exoskeleton" class="csl-entry" role="listitem">
Slade, P., Kochenderfer, M. J., Delp, S. L., &amp; Collins, S. H. (2022). Personalizing exoskeleton assistance while walking in the real world. <em>Nature</em>, <em>610</em>(7931), 277–282. <a href="https://doi.org/10.1038/s41586-022-05191-1">https://doi.org/10.1038/s41586-022-05191-1</a>
</div>
<div id="ref-Staunton2024_swimPCA" class="csl-entry" role="listitem">
Staunton, C. A., Romann, M., Björklund, G., &amp; Born, D.-P. (2024). Diving into a pool of data: Using principal component analysis to optimize performance prediction in women’s short-course swimming. <em>Journal of Sports Sciences</em>, <em>42</em>(6), 519–526. <a href="https://doi.org/10.1080/02640414.2024.2346670">https://doi.org/10.1080/02640414.2024.2346670</a>
</div>
<div id="ref-Uhlrich2023_OpenCap" class="csl-entry" role="listitem">
Uhlrich, S. D., Falisse, A., Kidziński, Łukasz, Muccini, J., Ko, M., Chaudhari, A. S., Hicks, J. L., &amp; Delp, S. L. (2023). OpenCap: Human movement dynamics from smartphone videos. <em>PLOS Computational Biology</em>, <em>19</em>(1), e1011462. <a href="https://doi.org/10.1371/journal.pcbi.1011462">https://doi.org/10.1371/journal.pcbi.1011462</a>
</div>
<div id="ref-Wang2025_SciData" class="csl-entry" role="listitem">
Wang, Y., Mei, Q., Jiang, H., Yang, X., Liew, B. X. W., Fernandez, J., &amp; Gu, Y. (2025). Dataset of walking and running biomechanics with different step widths across different speeds. <em>Scientific Data</em>, <em>12</em>(1), 802. <a href="https://doi.org/10.1038/s41597-025-05113-6">https://doi.org/10.1038/s41597-025-05113-6</a>
</div>
<div id="ref-Yamashita2025_fewshot" class="csl-entry" role="listitem">
Yamashita, D., Matsumoto, M., &amp; Matsubayashi, T. (2025). A proposed method for deep learning-based automatic tracking with minimal training data for sports biomechanics research. <em>Biomechanics (MDPI)</em>, <em>5</em>(2), 25. <a href="https://doi.org/10.3390/biomechanics5020025">https://doi.org/10.3390/biomechanics5020025</a>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>